{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjhFhu+rdROmm8EyUgZ4VD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evansmakori/Bankruptcy-in-Poland/blob/main/05_3_random_sampling_bankruptcy_in_poland.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Usage Guidelines\n",
        "\n",
        "This lesson is part of the DS Lab core curriculum. For that reason, this notebook can only be used on your WQU virtual machine.\n",
        "\n",
        "This means:\n",
        "\n",
        "‚ìß No downloading this notebook.\n",
        "‚ìß No re-sharing of this notebook with friends or colleagues.\n",
        "‚ìß No downloading the embedded videos in this notebook.\n",
        "‚ìß No re-sharing embedded videos with friends or colleagues.\n",
        "‚ìß No adding this notebook to public or private repositories.\n",
        "‚ìß No uploading this notebook (or screenshots of it) to other websites, including websites for study resources.\n",
        "5.3. Ensemble Models: Random Forest\n",
        "\n",
        "So far in this project, we've learned how to retrieve and decompress data, and how to manage imbalanced data to build a decision-tree model.\n",
        "\n",
        "In this lesson, we're going to expand our decision tree model into an entire forest (an example of something called an ensemble model); learn how to use a grid search to tune hyperparameters; and create a function that loads data and a pre-trained model, and uses that model to generate a Series of predictions.\n",
        "\n",
        "import gzip\n",
        "import json\n",
        "import pickle\n",
        "‚Äã\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import wqet_grader\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from IPython.display import VimeoVideo\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "‚Äã\n",
        "wqet_grader.init(\"Project 5 Assessment\")\n",
        "‚Äã\n",
        "VimeoVideo(\"694695674\", h=\"538b4d2725\", width=600)\n",
        "Prepare Data\n",
        "As always, we'll begin by importing the dataset.\n",
        "\n",
        "Import\n",
        "Task 5.3.1: Complete the wrangle function below using the code you developed in the lesson 5.1. Then use it to import poland-bankruptcy-data-2009.json.gz into the DataFrame df.\n",
        "\n",
        "Write a function in Python.WQU WorldQuant University Applied Data Science Lab QQQQ\n",
        "def wrangle(filename):\n",
        "    #open compressed file and load into dict\n",
        "    with gzip.open(filename, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    #turn dict into dataframe\n",
        "    df = pd.DataFrame().from_dict(data[\"data\"]).set_index(\"company_id\")\n",
        "    return df\n",
        "df = wrangle(\"data/poland-bankruptcy-data-2009.json.gz\")\n",
        "print(df.shape)\n",
        "df.head()\n",
        "Split\n",
        "Task 5.3.2: Create your feature matrix X and target vector y. Your target is \"bankrupt\".\n",
        "\n",
        "What's a feature matrix?\n",
        "What's a target vector?\n",
        "Subset a DataFrame by selecting one or more columns in pandas.\n",
        "Select a Series from a DataFrame in pandas.\n",
        "target = \"bankrupt\"\n",
        "X = df.drop(columns=target)\n",
        "y = df[target]\n",
        "‚Äã\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n",
        "Since we're not working with time series data, we're going to randomly divide our dataset into training and test sets ‚Äî just like we did in project 4.\n",
        "\n",
        "Task 5.3.3: Divide your data (X and y) into training and test sets using a randomized train-test split. Your test set should be 20% of your total data. And don't forget to set a random_state for reproducibility.\n",
        "\n",
        "Perform a randomized train-test split using scikit-learn.\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "‚Äã\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n",
        "You might have noticed that we didn't create a validation set, even though we're planning on tuning our model's hyperparameters in this lesson. That's because we're going to use cross-validation, which we'll talk about more later on.\n",
        "\n",
        "Resample\n",
        "VimeoVideo(\"694695662\", h=\"dc60d76861\", width=600)\n",
        "Task 5.3.4: Create a new feature matrix X_train_over and target vector y_train_over by performing random over-sampling on the training data.\n",
        "\n",
        "What is over-sampling?\n",
        "Perform random over-sampling using imbalanced-learn.\n",
        "over_sampler = RandomOverSampler(random_state=42)\n",
        "X_train_over, y_train_over = over_sampler.fit_resample(X_train, y_train)\n",
        "print(\"X_train_over shape:\", X_train_over.shape)\n",
        "X_train_over.head()\n",
        "Build Model\n",
        "Now that we have our data set up the right way, we can build the model. üèó\n",
        "\n",
        "Baseline\n",
        "Task 5.3.5: Calculate the baseline accuracy score for your model.\n",
        "\n",
        "What's accuracy score?\n",
        "Aggregate data in a Series using value_counts in pandas.\n",
        "acc_baseline = y_train.value_counts(normalize=True).max()\n",
        "print(\"Baseline Accuracy:\", round(acc_baseline, 4))\n",
        "Iterate\n",
        "So far, we've built single models that predict a single outcome. That's definitely a useful way to predict the future, but what if the one model we built isn't the right one? If we could somehow use more than one model simultaneously, we'd have a more trustworthy prediction.\n",
        "\n",
        "Ensemble models work by building multiple models on random subsets of the same data, and then comparing their predictions to make a final prediction. Since we used a decision tree in the last lesson, we're going to create an ensemble of trees here. This type of model is called a random forest.\n",
        "\n",
        "We'll start by creating a pipeline to streamline our workflow.\n",
        "\n",
        "VimeoVideo(\"694695643\", h=\"32c3d5b1ed\", width=600)\n",
        "Task 5.3.6: Create a pipeline named clf (short for \"classifier\") that contains a SimpleImputer transformer and a RandomForestClassifier predictor.\n",
        "\n",
        "What's an ensemble model?\n",
        "What's a random forest model?\n",
        "clf = make_pipeline(SimpleImputer(),RandomForestClassifier())\n",
        "print(clf)\n",
        "By default, the number of trees in our forest (n_estimators) is set to 100. That means when we train this classifier, we'll be fitting 100 trees. While it will take longer to train, it will hopefully lead to better performance.\n",
        "\n",
        "In order to get the best performance from our model, we need to tune its hyperparameter. But how can we do this if we haven't created a validation set? The answer is cross-validation. So, before we look at hyperparameters, let's see how cross-validation works with the classifier we just built.\n",
        "\n",
        "VimeoVideo(\"694695619\", h=\"2c41dca371\", width=600)\n",
        "Task 5.3.7: Perform cross-validation with your classifier, using the over-sampled training data. We want five folds, so set cv to 5. We also want to speed up training, to set n_jobs to -1.\n",
        "\n",
        "What's cross-validation?\n",
        "Perform k-fold cross-validation on a model in scikit-learn.\n",
        "cv_acc_scores = cross_val_score(clf, X_train_over, y_train_over, cv=5, n_jobs=-1)\n",
        "print(cv_acc_scores)\n",
        "That took kind of a long time, but we just trained 500 random forest classifiers (100 jobs x 5 folds). No wonder it takes so long!\n",
        "\n",
        "Pro tip: even though cross_val_score is useful for getting an idea of how cross-validation works, you'll rarely use it. Instead, most people include a cv argument when they do a hyperparameter search.\n",
        "\n",
        "Now that we have an idea of how cross-validation works, let's tune our model. The first step is creating a range of hyperparameters that we want to evaluate.\n",
        "\n",
        "VimeoVideo(\"694695593\", h=\"5143f0b63f\", width=600)\n",
        "Task 5.3.8: Create a dictionary with the range of hyperparameters that we want to evaluate for our classifier.\n",
        "\n",
        "For the SimpleImputer, try both the \"mean\" and \"median\" strategies.\n",
        "For the RandomForestClassifier, try max_depth settings between 10 and 50, by steps of 10.\n",
        "Also for the RandomForestClassifier, try n_estimators settings between 25 and 100 by steps of 25.\n",
        "What's a dictionary?\n",
        "What's a hyperparameter?\n",
        "Create a range in Python\n",
        "Define a hyperparameter grid for model tuning in scikit-learn.\n",
        "params = {\n",
        "    \"simpleimputer__strategy\": [\"mean\", \"median\"],\n",
        "    \"randomforestclassifier__max_depth\": range(10, 50, 10),\n",
        "    \"randomforestclassifier__n_estimators\": range(25, 100, 25)\n",
        "\n",
        "}\n",
        "params\n",
        "Now that we have our hyperparameter grid, let's incorporate it into a grid search.\n",
        "\n",
        "VimeoVideo(\"694695574\", h=\"8588bf015f\", width=600)\n",
        "Task 5.3.9: Create a GridSearchCV named model that includes your classifier and hyperparameter grid. Be sure to use the same arguments for cv and n_jobs that you used above, and set verbose to 1.\n",
        "\n",
        "What's cross-validation?\n",
        "What's a grid search?\n",
        "Perform a hyperparameter grid search in scikit-learn.\n",
        "model = GridSearchCV(\n",
        "    clf,\n",
        "    param_grid=params,\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "model\n",
        "Finally, now let's fit the model.\n",
        "\n",
        "VimeoVideo(\"694695566\", h=\"f4e9910a9e\", width=600)\n",
        "Task 5.3.10: Fit model to the over-sampled training data.\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train_over, y_train_over)\n",
        "This will take some time to train, so let's take a moment to think about why. How many forests did we just test? 4 different max_depths times 3 n_estimators times 2 imputation strategies... that makes 24 forests. How many fits did we just do? 24 forests times 5 folds is 120. And remember that each forest is comprised of 25-75 trees, which works out to at least 3,000 trees. So it's computationally expensive!\n",
        "\n",
        "Okay, now that we've tested all those models, let's take a look at the results.\n",
        "\n",
        "VimeoVideo(\"694695546\", h=\"4ae60129c4\", width=600)\n",
        "Task 5.3.11: Extract the cross-validation results from model and load them into a DataFrame named cv_results.\n",
        "\n",
        "Get cross-validation results from a hyperparameter search in scikit-learn.\n",
        "cv_results = pd.DataFrame(model.cv_results_)\n",
        "cv_results.head(10)\n",
        "In addition to the accuracy scores for all the different models we tried during our grid search, we can see how long it took each model to train. Let's take a closer look at how different hyperparameter settings affect training time.\n",
        "\n",
        "First, we'll look at n_estimators. Our grid search evaluated this hyperparameter for various max_depth settings, but let's only look at models where max_depth equals 10.\n",
        "\n",
        "VimeoVideo(\"694695537\", h=\"e460435664\", width=600)\n",
        "Task 5.3.12: Create a mask for cv_results for rows where \"param_randomforestclassifier__max_depth\" equals 10. Then plot \"param_randomforestclassifier__n_estimators\" on the x-axis and \"mean_fit_time\" on the y-axis. Don't forget to label your axes and include a title.\n",
        "\n",
        "Subset a DataFrame with a mask using pandas.\n",
        "Create a line plot in Matplotlib.\n",
        "# Create mask\n",
        "mask = cv_results[\"param_randomforestclassifier__max_depth\"]==10\n",
        "# Plot fit time vs n_estimators\n",
        "plt.plot(\n",
        "    cv_results[mask][\"param_randomforestclassifier__n_estimators\"],\n",
        "    cv_results[mask][\"mean_fit_time\"]\n",
        ")\n",
        "# Label axes\n",
        "plt.xlabel(\"Number of Estimators\")\n",
        "plt.ylabel(\"Mean Fit Time [seconds]\")\n",
        "plt.title(\"Training Time vs Estimators (max_depth=10)\");\n",
        "‚Äã\n",
        "Next, we'll look at max_depth. Here, we'll also limit our data to rows where n_estimators equals 25.\n",
        "\n",
        "VimeoVideo(\"694695525\", h=\"99f2dfc9eb\", width=600)\n",
        "Task 5.3.13: Create a mask for cv_results for rows where \"param_randomforestclassifier__n_estimators\" equals 25. Then plot \"param_randomforestclassifier__max_depth\" on the x-axis and \"mean_fit_time\" on the y-axis. Don't forget to label your axes and include a title.\n",
        "\n",
        "Subset a DataFrame with a mask using pandas.\n",
        "Create a line plot in Matplotlib.\n",
        "# Create mask\n",
        "mask = cv_results[\"param_randomforestclassifier__n_estimators\"]==25\n",
        "# Plot fit time vs max_depth\n",
        "plt.plot(\n",
        "    cv_results[mask][\"param_randomforestclassifier__max_depth\"],\n",
        "    cv_results[mask][\"mean_fit_time\"]\n",
        ")\n",
        "# Label axes\n",
        "plt.xlabel(\"Max Depth\")\n",
        "plt.ylabel(\"Mean Fit Time [seconds]\")\n",
        "plt.title(\"Training Time vs Max Depth (n_estimators=25)\");\n",
        "‚Äã\n",
        "There's a general upwards trend, but we see a lot of up-and-down here. That's because for each max depth, grid search tries two different imputation strategies: mean and median. Median is a lot faster to calculate, so that speeds up training time.\n",
        "\n",
        "Finally, let's look at the hyperparameters that led to the best performance.\n",
        "\n",
        "VimeoVideo(\"694695505\", h=\"f98f660ce1\", width=600)\n",
        "Task 5.3.14: Extract the best hyperparameters from model.\n",
        "\n",
        "Get settings from a hyperparameter search in scikit-learn.\n",
        "‚Äã\n",
        "# Extract best hyperparameters\n",
        "model.predict(X_train_over)\n",
        "‚Äã\n",
        "Note that we don't need to build and train a new model with these settings. Now that the grid search is complete, when we use model.predict(), it will serve up predictions using the best model ‚Äî something that we'll do at the end of this lesson.\n",
        "\n",
        "Evaluate\n",
        "All right: The moment of truth. Let's see how our model performs.\n",
        "\n",
        "Task 5.3.15: Calculate the training and test accuracy scores for model.\n",
        "\n",
        "Calculate the accuracy score for a model in scikit-learn.\n",
        "acc_train = model.score(X_train, y_train)\n",
        "acc_test = model.score(X_test, y_test)\n",
        "‚Äã\n",
        "print(\"Training Accuracy:\", round(acc_train, 4))\n",
        "print(\"Test Accuracy:\", round(acc_test, 4))\n",
        "‚Äã\n",
        "We beat the baseline! Just barely, but we beat it.\n",
        "\n",
        "Next, we're going to use a confusion matrix to see how our model performs. To better understand the values we'll see in the matrix, let's first count how many observations in our test set belong to the positive and negative classes.\n",
        "\n",
        "y_test.value_counts()\n",
        "VimeoVideo(\"694695486\", h=\"1d6ac2bf77\", width=600)\n",
        "Task 5.3.16: Plot a confusion matrix that shows how your best model performs on your test set.\n",
        "\n",
        "What's a confusion matrix?\n",
        "Create a confusion matrix using scikit-learn.\n",
        "# Plot confusion matrix\n",
        "# Plot confusion matrix\n",
        "ConfusionMatrixDisplay.from_estimator(model, X_test, y_test);\n",
        "‚Äã\n",
        "Notice the relationship between the numbers in this matrix with the count you did the previous task. If you sum the values in the bottom row, you get the total number of positive observations in y_test ( 72+11=83\n",
        " ). And the top row sum to the number of negative observations ( 1903+10=1913\n",
        " ).\n",
        "\n",
        "Communicate\n",
        "VimeoVideo(\"698358615\", h=\"3fd4b2186a\", width=600)\n",
        "Task 5.3.17: Create a horizontal bar chart with the 10 most important features for your model.\n",
        "\n",
        "# Get feature names from training data\n",
        "features =X_train_over.columns\n",
        "# Extract importances from model\n",
        "importances = model.best_estimator_.named_steps[\n",
        "    \"randomforestclassifier\"\n",
        "].feature_importances_\n",
        "# Create a series with feature names and importances\n",
        "feat_imp = pd.Series(importances, index=features).sort_values()\n",
        "# Plot 10 most important features\n",
        "feat_imp.tail(10).plot(kind=\"barh\")\n",
        "plt.xlabel(\"Gini Importance\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(\"Feature Importance\");\n",
        "‚Äã\n",
        "The only thing left now is to save your model so that it can be reused.\n",
        "\n",
        "VimeoVideo(\"694695478\", h=\"a13bdacb55\", width=600)\n",
        "Task 5.3.18: Using a context manager, save your best-performing model to a a file named \"model-5-3.pkl\".\n",
        "\n",
        "What's serialization?\n",
        "Store a Python object as a serialized file using pickle.\n",
        "# Save model\n",
        "# Save model\n",
        "with open(\"model-5-3.pkl\", \"wb\") as f:\n",
        "    pickle.dump(model, f)\n",
        "VimeoVideo(\"694695451\", h=\"fc96dd8d1f\", width=600)\n",
        "Task 5.3.19: Create a function make_predictions. It should take two arguments: the path of a JSON file that contains test data and the path of a serialized model. The function should load and clean the data using the wrangle function you created, load the model, generate an array of predictions, and convert that array into a Series. (The Series should have the name \"bankrupt\" and the same index labels as the test data.) Finally, the function should return its predictions as a Series.\n",
        "\n",
        "What's a function?\n",
        "Load a serialized file\n",
        "What's a Series?\n",
        "Create a Series in pandas\n",
        "def make_predictions(data_filepath, model_filepath):\n",
        "    # Wrangle JSON file\n",
        "    X_test = wrangle(data_filepath)\n",
        "    # Load model\n",
        "    with open(model_filepath, \"rb\") as f:\n",
        "        model = pickle.load(f)\n",
        "\n",
        "    # Generate predictions\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    # Put predictions into Series with name \"bankrupt\", and same index as X_test\n",
        "    y_test_pred = pd.Series(y_test_pred, index=X_test.index, name=\"bankrupt\")\n",
        "    return y_test_pred\n",
        "VimeoVideo(\"694695426\", h=\"f75588d43a\", width=600)\n",
        "Task 5.3.20: Use the code below to check your make_predictions function. Once you're satisfied with the result, submit it to the grader.\n",
        "\n",
        "y_test_pred = make_predictions(\n",
        "    data_filepath=\"data/poland-bankruptcy-data-2009-mvp-features.json.gz\",\n",
        "    model_filepath=\"model-5-3.pkl\",\n",
        ")\n",
        "‚Äã\n",
        "print(\"predictions shape:\", y_test_pred.shape)\n",
        "y_test_pred.head()\n",
        "wqet_grader.grade(\n",
        "    \"Project 5 Assessment\",\n",
        "    \"Task 5.3.19\",\n",
        "    make_predictions(\n",
        "        data_filepath=\"data/poland-bankruptcy-data-2009-mvp-features.json.gz\",\n",
        "        model_filepath=\"model-5-3.pkl\",\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "ehm0ZNFZHWqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usage Guidelines\n",
        "\n",
        "This lesson is part of the DS Lab core curriculum. For that reason, this notebook can only be used on your WQU virtual machine.\n",
        "\n",
        "This means:\n",
        "\n",
        "‚ìß No downloading this notebook.\n",
        "‚ìß No re-sharing of this notebook with friends or colleagues.\n",
        "‚ìß No downloading the embedded videos in this notebook.\n",
        "‚ìß No re-sharing embedded videos with friends or colleagues.\n",
        "‚ìß No adding this notebook to public or private repositories.\n",
        "‚ìß No uploading this notebook (or screenshots of it) to other websites, including websites for study resources.\n",
        "5.3. Ensemble Models: Random Forest\n",
        "\n",
        "So far in this project, we've learned how to retrieve and decompress data, and how to manage imbalanced data to build a decision-tree model.\n",
        "\n",
        "In this lesson, we're going to expand our decision tree model into an entire forest (an example of something called an ensemble model); learn how to use a grid search to tune hyperparameters; and create a function that loads data and a pre-trained model, and uses that model to generate a Series of predictions.\n",
        "\n",
        "import gzip\n",
        "import json\n",
        "import pickle\n",
        "‚Äã\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import wqet_grader\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from IPython.display import VimeoVideo\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "‚Äã\n",
        "wqet_grader.init(\"Project 5 Assessment\")\n",
        "‚Äã\n",
        "VimeoVideo(\"694695674\", h=\"538b4d2725\", width=600)\n",
        "Prepare Data\n",
        "As always, we'll begin by importing the dataset.\n",
        "\n",
        "Import\n",
        "Task 5.3.1: Complete the wrangle function below using the code you developed in the lesson 5.1. Then use it to import poland-bankruptcy-data-2009.json.gz into the DataFrame df.\n",
        "\n",
        "Write a function in Python.WQU WorldQuant University Applied Data Science Lab QQQQ\n",
        "def wrangle(filename):\n",
        "    #open compressed file and load into dict\n",
        "    with gzip.open(filename, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    #turn dict into dataframe\n",
        "    df = pd.DataFrame().from_dict(data[\"data\"]).set_index(\"company_id\")\n",
        "    return df\n",
        "df = wrangle(\"data/poland-bankruptcy-data-2009.json.gz\")\n",
        "print(df.shape)\n",
        "df.head()\n",
        "Split\n",
        "Task 5.3.2: Create your feature matrix X and target vector y. Your target is \"bankrupt\".\n",
        "\n",
        "What's a feature matrix?\n",
        "What's a target vector?\n",
        "Subset a DataFrame by selecting one or more columns in pandas.\n",
        "Select a Series from a DataFrame in pandas.\n",
        "target = \"bankrupt\"\n",
        "X = df.drop(columns=target)\n",
        "y = df[target]\n",
        "‚Äã\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n",
        "Since we're not working with time series data, we're going to randomly divide our dataset into training and test sets ‚Äî just like we did in project 4.\n",
        "\n",
        "Task 5.3.3: Divide your data (X and y) into training and test sets using a randomized train-test split. Your test set should be 20% of your total data. And don't forget to set a random_state for reproducibility.\n",
        "\n",
        "Perform a randomized train-test split using scikit-learn.\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "‚Äã\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n",
        "You might have noticed that we didn't create a validation set, even though we're planning on tuning our model's hyperparameters in this lesson. That's because we're going to use cross-validation, which we'll talk about more later on.\n",
        "\n",
        "Resample\n",
        "VimeoVideo(\"694695662\", h=\"dc60d76861\", width=600)\n",
        "Task 5.3.4: Create a new feature matrix X_train_over and target vector y_train_over by performing random over-sampling on the training data.\n",
        "\n",
        "What is over-sampling?\n",
        "Perform random over-sampling using imbalanced-learn.\n",
        "over_sampler = RandomOverSampler(random_state=42)\n",
        "X_train_over, y_train_over = over_sampler.fit_resample(X_train, y_train)\n",
        "print(\"X_train_over shape:\", X_train_over.shape)\n",
        "X_train_over.head()\n",
        "Build Model\n",
        "Now that we have our data set up the right way, we can build the model. üèó\n",
        "\n",
        "Baseline\n",
        "Task 5.3.5: Calculate the baseline accuracy score for your model.\n",
        "\n",
        "What's accuracy score?\n",
        "Aggregate data in a Series using value_counts in pandas.\n",
        "acc_baseline = y_train.value_counts(normalize=True).max()\n",
        "print(\"Baseline Accuracy:\", round(acc_baseline, 4))\n",
        "Iterate\n",
        "So far, we've built single models that predict a single outcome. That's definitely a useful way to predict the future, but what if the one model we built isn't the right one? If we could somehow use more than one model simultaneously, we'd have a more trustworthy prediction.\n",
        "\n",
        "Ensemble models work by building multiple models on random subsets of the same data, and then comparing their predictions to make a final prediction. Since we used a decision tree in the last lesson, we're going to create an ensemble of trees here. This type of model is called a random forest.\n",
        "\n",
        "We'll start by creating a pipeline to streamline our workflow.\n",
        "\n",
        "VimeoVideo(\"694695643\", h=\"32c3d5b1ed\", width=600)\n",
        "Task 5.3.6: Create a pipeline named clf (short for \"classifier\") that contains a SimpleImputer transformer and a RandomForestClassifier predictor.\n",
        "\n",
        "What's an ensemble model?\n",
        "What's a random forest model?\n",
        "clf = make_pipeline(SimpleImputer(),RandomForestClassifier())\n",
        "print(clf)\n",
        "By default, the number of trees in our forest (n_estimators) is set to 100. That means when we train this classifier, we'll be fitting 100 trees. While it will take longer to train, it will hopefully lead to better performance.\n",
        "\n",
        "In order to get the best performance from our model, we need to tune its hyperparameter. But how can we do this if we haven't created a validation set? The answer is cross-validation. So, before we look at hyperparameters, let's see how cross-validation works with the classifier we just built.\n",
        "\n",
        "VimeoVideo(\"694695619\", h=\"2c41dca371\", width=600)\n",
        "Task 5.3.7: Perform cross-validation with your classifier, using the over-sampled training data. We want five folds, so set cv to 5. We also want to speed up training, to set n_jobs to -1.\n",
        "\n",
        "What's cross-validation?\n",
        "Perform k-fold cross-validation on a model in scikit-learn.\n",
        "cv_acc_scores = cross_val_score(clf, X_train_over, y_train_over, cv=5, n_jobs=-1)\n",
        "print(cv_acc_scores)\n",
        "That took kind of a long time, but we just trained 500 random forest classifiers (100 jobs x 5 folds). No wonder it takes so long!\n",
        "\n",
        "Pro tip: even though cross_val_score is useful for getting an idea of how cross-validation works, you'll rarely use it. Instead, most people include a cv argument when they do a hyperparameter search.\n",
        "\n",
        "Now that we have an idea of how cross-validation works, let's tune our model. The first step is creating a range of hyperparameters that we want to evaluate.\n",
        "\n",
        "VimeoVideo(\"694695593\", h=\"5143f0b63f\", width=600)\n",
        "Task 5.3.8: Create a dictionary with the range of hyperparameters that we want to evaluate for our classifier.\n",
        "\n",
        "For the SimpleImputer, try both the \"mean\" and \"median\" strategies.\n",
        "For the RandomForestClassifier, try max_depth settings between 10 and 50, by steps of 10.\n",
        "Also for the RandomForestClassifier, try n_estimators settings between 25 and 100 by steps of 25.\n",
        "What's a dictionary?\n",
        "What's a hyperparameter?\n",
        "Create a range in Python\n",
        "Define a hyperparameter grid for model tuning in scikit-learn.\n",
        "params = {\n",
        "    \"simpleimputer__strategy\": [\"mean\", \"median\"],\n",
        "    \"randomforestclassifier__max_depth\": range(10, 50, 10),\n",
        "    \"randomforestclassifier__n_estimators\": range(25, 100, 25)\n",
        "    \n",
        "}\n",
        "params\n",
        "Now that we have our hyperparameter grid, let's incorporate it into a grid search.\n",
        "\n",
        "VimeoVideo(\"694695574\", h=\"8588bf015f\", width=600)\n",
        "Task 5.3.9: Create a GridSearchCV named model that includes your classifier and hyperparameter grid. Be sure to use the same arguments for cv and n_jobs that you used above, and set verbose to 1.\n",
        "\n",
        "What's cross-validation?\n",
        "What's a grid search?\n",
        "Perform a hyperparameter grid search in scikit-learn.\n",
        "model = GridSearchCV(\n",
        "    clf,\n",
        "    param_grid=params,\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "model\n",
        "Finally, now let's fit the model.\n",
        "\n",
        "VimeoVideo(\"694695566\", h=\"f4e9910a9e\", width=600)\n",
        "Task 5.3.10: Fit model to the over-sampled training data.\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train_over, y_train_over)\n",
        "This will take some time to train, so let's take a moment to think about why. How many forests did we just test? 4 different max_depths times 3 n_estimators times 2 imputation strategies... that makes 24 forests. How many fits did we just do? 24 forests times 5 folds is 120. And remember that each forest is comprised of 25-75 trees, which works out to at least 3,000 trees. So it's computationally expensive!\n",
        "\n",
        "Okay, now that we've tested all those models, let's take a look at the results.\n",
        "\n",
        "VimeoVideo(\"694695546\", h=\"4ae60129c4\", width=600)\n",
        "Task 5.3.11: Extract the cross-validation results from model and load them into a DataFrame named cv_results.\n",
        "\n",
        "Get cross-validation results from a hyperparameter search in scikit-learn.\n",
        "cv_results = pd.DataFrame(model.cv_results_)\n",
        "cv_results.head(10)\n",
        "In addition to the accuracy scores for all the different models we tried during our grid search, we can see how long it took each model to train. Let's take a closer look at how different hyperparameter settings affect training time.\n",
        "\n",
        "First, we'll look at n_estimators. Our grid search evaluated this hyperparameter for various max_depth settings, but let's only look at models where max_depth equals 10.\n",
        "\n",
        "VimeoVideo(\"694695537\", h=\"e460435664\", width=600)\n",
        "Task 5.3.12: Create a mask for cv_results for rows where \"param_randomforestclassifier__max_depth\" equals 10. Then plot \"param_randomforestclassifier__n_estimators\" on the x-axis and \"mean_fit_time\" on the y-axis. Don't forget to label your axes and include a title.\n",
        "\n",
        "Subset a DataFrame with a mask using pandas.\n",
        "Create a line plot in Matplotlib.\n",
        "# Create mask\n",
        "mask = cv_results[\"param_randomforestclassifier__max_depth\"]==10\n",
        "# Plot fit time vs n_estimators\n",
        "plt.plot(\n",
        "    cv_results[mask][\"param_randomforestclassifier__n_estimators\"],\n",
        "    cv_results[mask][\"mean_fit_time\"]\n",
        ")\n",
        "# Label axes\n",
        "plt.xlabel(\"Number of Estimators\")\n",
        "plt.ylabel(\"Mean Fit Time [seconds]\")\n",
        "plt.title(\"Training Time vs Estimators (max_depth=10)\");\n",
        "‚Äã\n",
        "Next, we'll look at max_depth. Here, we'll also limit our data to rows where n_estimators equals 25.\n",
        "\n",
        "VimeoVideo(\"694695525\", h=\"99f2dfc9eb\", width=600)\n",
        "Task 5.3.13: Create a mask for cv_results for rows where \"param_randomforestclassifier__n_estimators\" equals 25. Then plot \"param_randomforestclassifier__max_depth\" on the x-axis and \"mean_fit_time\" on the y-axis. Don't forget to label your axes and include a title.\n",
        "\n",
        "Subset a DataFrame with a mask using pandas.\n",
        "Create a line plot in Matplotlib.\n",
        "# Create mask\n",
        "mask = cv_results[\"param_randomforestclassifier__n_estimators\"]==25\n",
        "# Plot fit time vs max_depth\n",
        "plt.plot(\n",
        "    cv_results[mask][\"param_randomforestclassifier__max_depth\"],\n",
        "    cv_results[mask][\"mean_fit_time\"]\n",
        ")\n",
        "# Label axes\n",
        "plt.xlabel(\"Max Depth\")\n",
        "plt.ylabel(\"Mean Fit Time [seconds]\")\n",
        "plt.title(\"Training Time vs Max Depth (n_estimators=25)\");\n",
        "‚Äã\n",
        "There's a general upwards trend, but we see a lot of up-and-down here. That's because for each max depth, grid search tries two different imputation strategies: mean and median. Median is a lot faster to calculate, so that speeds up training time.\n",
        "\n",
        "Finally, let's look at the hyperparameters that led to the best performance.\n",
        "\n",
        "VimeoVideo(\"694695505\", h=\"f98f660ce1\", width=600)\n",
        "Task 5.3.14: Extract the best hyperparameters from model.\n",
        "\n",
        "Get settings from a hyperparameter search in scikit-learn.\n",
        "‚Äã\n",
        "# Extract best hyperparameters\n",
        "model.predict(X_train_over)\n",
        "‚Äã\n",
        "Note that we don't need to build and train a new model with these settings. Now that the grid search is complete, when we use model.predict(), it will serve up predictions using the best model ‚Äî something that we'll do at the end of this lesson.\n",
        "\n",
        "Evaluate\n",
        "All right: The moment of truth. Let's see how our model performs.\n",
        "\n",
        "Task 5.3.15: Calculate the training and test accuracy scores for model.\n",
        "\n",
        "Calculate the accuracy score for a model in scikit-learn.\n",
        "acc_train = model.score(X_train, y_train)\n",
        "acc_test = model.score(X_test, y_test)\n",
        "‚Äã\n",
        "print(\"Training Accuracy:\", round(acc_train, 4))\n",
        "print(\"Test Accuracy:\", round(acc_test, 4))\n",
        "‚Äã\n",
        "We beat the baseline! Just barely, but we beat it.\n",
        "\n",
        "Next, we're going to use a confusion matrix to see how our model performs. To better understand the values we'll see in the matrix, let's first count how many observations in our test set belong to the positive and negative classes.\n",
        "\n",
        "y_test.value_counts()\n",
        "VimeoVideo(\"694695486\", h=\"1d6ac2bf77\", width=600)\n",
        "Task 5.3.16: Plot a confusion matrix that shows how your best model performs on your test set.\n",
        "\n",
        "What's a confusion matrix?\n",
        "Create a confusion matrix using scikit-learn.\n",
        "# Plot confusion matrix\n",
        "# Plot confusion matrix\n",
        "ConfusionMatrixDisplay.from_estimator(model, X_test, y_test);\n",
        "‚Äã\n",
        "Notice the relationship between the numbers in this matrix with the count you did the previous task. If you sum the values in the bottom row, you get the total number of positive observations in y_test ( 72+11=83\n",
        " ). And the top row sum to the number of negative observations ( 1903+10=1913\n",
        " ).\n",
        "\n",
        "Communicate\n",
        "VimeoVideo(\"698358615\", h=\"3fd4b2186a\", width=600)\n",
        "Task 5.3.17: Create a horizontal bar chart with the 10 most important features for your model.\n",
        "\n",
        "# Get feature names from training data\n",
        "features =X_train_over.columns\n",
        "# Extract importances from model\n",
        "importances = model.best_estimator_.named_steps[\n",
        "    \"randomforestclassifier\"\n",
        "].feature_importances_\n",
        "# Create a series with feature names and importances\n",
        "feat_imp = pd.Series(importances, index=features).sort_values()\n",
        "# Plot 10 most important features\n",
        "feat_imp.tail(10).plot(kind=\"barh\")\n",
        "plt.xlabel(\"Gini Importance\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(\"Feature Importance\");\n",
        "‚Äã\n",
        "The only thing left now is to save your model so that it can be reused.\n",
        "\n",
        "VimeoVideo(\"694695478\", h=\"a13bdacb55\", width=600)\n",
        "Task 5.3.18: Using a context manager, save your best-performing model to a a file named \"model-5-3.pkl\".\n",
        "\n",
        "What's serialization?\n",
        "Store a Python object as a serialized file using pickle.\n",
        "# Save model\n",
        "# Save model\n",
        "with open(\"model-5-3.pkl\", \"wb\") as f:\n",
        "    pickle.dump(model, f)\n",
        "VimeoVideo(\"694695451\", h=\"fc96dd8d1f\", width=600)\n",
        "Task 5.3.19: Create a function make_predictions. It should take two arguments: the path of a JSON file that contains test data and the path of a serialized model. The function should load and clean the data using the wrangle function you created, load the model, generate an array of predictions, and convert that array into a Series. (The Series should have the name \"bankrupt\" and the same index labels as the test data.) Finally, the function should return its predictions as a Series.\n",
        "\n",
        "What's a function?\n",
        "Load a serialized file\n",
        "What's a Series?\n",
        "Create a Series in pandas\n",
        "def make_predictions(data_filepath, model_filepath):\n",
        "    # Wrangle JSON file\n",
        "    X_test = wrangle(data_filepath)\n",
        "    # Load model\n",
        "    with open(model_filepath, \"rb\") as f:\n",
        "        model = pickle.load(f)\n",
        "    \n",
        "    # Generate predictions\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    # Put predictions into Series with name \"bankrupt\", and same index as X_test\n",
        "    y_test_pred = pd.Series(y_test_pred, index=X_test.index, name=\"bankrupt\")\n",
        "    return y_test_pred\n",
        "VimeoVideo(\"694695426\", h=\"f75588d43a\", width=600)\n",
        "Task 5.3.20: Use the code below to check your make_predictions function. Once you're satisfied with the result, submit it to the grader.\n",
        "\n",
        "y_test_pred = make_predictions(\n",
        "    data_filepath=\"data/poland-bankruptcy-data-2009-mvp-features.json.gz\",\n",
        "    model_filepath=\"model-5-3.pkl\",\n",
        ")\n",
        "‚Äã\n",
        "print(\"predictions shape:\", y_test_pred.shape)\n",
        "y_test_pred.head()\n",
        "wqet_grader.grade(\n",
        "    \"Project 5 Assessment\",\n",
        "    \"Task 5.3.19\",\n",
        "    make_predictions(\n",
        "        data_filepath=\"data/poland-bankruptcy-data-2009-mvp-features.json.gz\",\n",
        "        model_filepath=\"model-5-3.pkl\",\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "pPJejSq9Ha5K"
      }
    }
  ]
}