{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvwMcOi2NTO1tFTIMVeJmH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evansmakori/Bankruptcy-in-Poland/blob/main/05_4_bankruptcy_in_poland_Gradient_Boosting_Trees_lesson_4_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.4. Gradient Boosting Trees\n",
        "\n",
        "You've been working hard, and now you have all the tools you need to build and tune models. We'll start this lesson the same way we've started the others: preparing the data and building our model, and this time with a new ensemble model. Once it's working, we'll learn some new performance metrics to evaluate it. By the end of this lesson, you'll have written your first Python module!"
      ],
      "metadata": {
        "id": "_bVbMZQmedg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "import ipywidgets as widgets\n",
        "import pandas as pd\n",
        "import wqet_grader\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from IPython.display import VimeoVideo\n",
        "from ipywidgets import interact\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import (\n",
        "    ConfusionMatrixDisplay,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        ")\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from teaching_tools.widgets import ConfusionMatrixWidget\n",
        "\n",
        "wqet_grader.init(\"Project 5 Assessment\")"
      ],
      "metadata": {
        "id": "qTL_NDc-egSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare Data\n",
        "All the data preparation for this module is the same as it was last time around. See you on the other side!\n",
        "\n",
        "Import\n",
        "Task 5.4.1: Complete the wrangle function below using the code you developed in the lesson 5.1. Then use it to import poland-bankruptcy-data-2009.json.gz into the DataFrame df"
      ],
      "metadata": {
        "id": "qxcCCsgieip3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wrangle(filename):\n",
        "    #open compressed file and load into dict\n",
        "    with gzip.open(filename,\"r\") as f:\n",
        "        data= json.load(f)\n",
        "\n",
        "    #tunring dict into DataFrame\n",
        "    df=pd.DataFrame().from_dict(data[\"data\"]).set_index(\"company_id\")\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "-UeDMwirekBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = wrangle(\"data/poland-bankruptcy-data-2009.json.gz\")\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "bDSvxNswek-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split\n",
        "Task 5.4.2: Create your feature matrix X and target vector y. Your target is \"bankrupt\"."
      ],
      "metadata": {
        "id": "dSrazlDwemeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target = \"bankrupt\"\n",
        "X = df.drop(columns=target)\n",
        "y = df[target]\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)"
      ],
      "metadata": {
        "id": "4X29HGfqenqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 5.4.3: Divide your data (X and y) into training and test sets using a randomized train-test split. Your test set should be 20% of your total data. And don't forget to set a random_state for reproducibility."
      ],
      "metadata": {
        "id": "kTmuxl3nepqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ],
      "metadata": {
        "id": "vUCMLjK5eqZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resample\n",
        "Task 5.4.4: Create a new feature matrix X_train_over and target vector y_train_over by performing random over-sampling on the training data."
      ],
      "metadata": {
        "id": "VmqpXg3EesHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "over_sampler = RandomOverSampler(random_state=42)\n",
        "X_train_over, y_train_over = over_sampler.fit_resample(X_train,y_train)\n",
        "print(\"X_train_over shape:\", X_train_over.shape)\n",
        "X_train_over.head()"
      ],
      "metadata": {
        "id": "kZKmtgEmetWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build Model\n",
        "Now let's put together our model. We'll start by calculating the baseline accuracy, just like we did last time.\n",
        "\n",
        "Baseline\n",
        "Task 5.4.5: Calculate the baseline accuracy score for your model."
      ],
      "metadata": {
        "id": "2NmOj-8JevvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_baseline = y_train.value_counts(normalize=True).max()\n",
        "print(\"Baseline Accuracy:\", round(acc_baseline, 4))"
      ],
      "metadata": {
        "id": "KG_1kPRsexFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iterate\n",
        "Even though the building blocks are the same, here's where we start working with something new. First, we're going to use a new type of ensemble model for our classifier."
      ],
      "metadata": {
        "id": "2RNsv40_ezXE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 5.4.6: Create a pipeline named clf (short for \"classifier\") that contains a SimpleImputer transformer and a GradientBoostingClassifier predictor."
      ],
      "metadata": {
        "id": "qfzMWfI3e01c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = make_pipeline(\n",
        "    SimpleImputer(), GradientBoostingClassifier()\n",
        ")"
      ],
      "metadata": {
        "id": "Y4Yi1cObe1vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1Ly5dvtTe5Cr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember while we're doing this that we only want to be looking at the positive class. Here, the positive class is the one where the companies really did go bankrupt. In the dictionary we made last time, the positive class is made up of the companies with the bankrupt: true key-value pair.\n",
        "\n",
        "Next, we're going to tune some of the hyperparameters for our model.\n",
        "\n",
        "VimeoVideo(\"696221055\", h=\"b675d7fec0\", width=600)\n",
        "Task 5.4.7: Create a dictionary with the range of hyperparameters that we want to evaluate for our classifier.\n",
        "\n",
        "For the SimpleImputer, try both the \"mean\" and \"median\" strategies.\n",
        "For the GradientBoostingClassifier, try max_depth settings between 2 and 5.\n",
        "Also for the GradientBoostingClassifier, try n_estimators settings between 20 and 31, by steps of 5."
      ],
      "metadata": {
        "id": "F5hb9yj2e5LF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    \"simpleimputer__strategy\": [\"mean\", \"median\"],\n",
        "    \"gradientboostingclassifier__n_estimators\": range(20,30,5),\n",
        "    \"gradientboostingclassifier__max_depth\": range(2,5)\n",
        "}\n",
        "params"
      ],
      "metadata": {
        "id": "-OfAPq8Pe8Ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we're trying much smaller numbers of n_estimators. This is because GradientBoostingClassifier is slower to train than the RandomForestClassifier. You can try increasing the number of estimators to see if model performance improves, but keep in mind that you could be waiting a long time!"
      ],
      "metadata": {
        "id": "k10MJFC9e9VF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 5.4.8: Create a GridSearchCV named model that includes your classifier and hyperparameter grid. Be sure to use the same arguments for cv and n_jobs that you used above, and set verbose to 1."
      ],
      "metadata": {
        "id": "awl_Ixiae-xQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GridSearchCV(clf, param_grid=params, cv=5, n_jobs=-1, verbose=1)"
      ],
      "metadata": {
        "id": "wnJn8Zate_3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 5.4.9: Fit your model to the over-sampled training data."
      ],
      "metadata": {
        "id": "QqL5fhOMfB5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit model to over-sampled training data\n",
        "model.fit(X_train_over, y_train_over)"
      ],
      "metadata": {
        "id": "iJYw0VlIfC5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will take longer than our last grid search, so now's a good time to get coffee or cook dinner. 🍲\n",
        "\n",
        "Okay! Let's take a look at the results!"
      ],
      "metadata": {
        "id": "zu-KQoLJfEWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 5.4.10: Extract the cross-validation results from model and load them into a DataFrame named cv_results.\n",
        "\n",
        "Get cross-validation results from a hyperparameter search in scikit-learn."
      ],
      "metadata": {
        "id": "ANpxZdbefF7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame(model.cv_results_)\n",
        "results.sort_values(\"rank_test_score\").head(10)"
      ],
      "metadata": {
        "id": "vWEvURhyfG3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "E5gFwvNFfI6g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 5.4.11: Extract the best hyperparameters from model."
      ],
      "metadata": {
        "id": "FrxRW15MfJDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract best hyperparameters\n",
        "model.best_params_"
      ],
      "metadata": {
        "id": "HWdLZC15fKvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate\n",
        "Now that we have a working model that's actually giving us something useful, let's see how good it really is.\n",
        "\n",
        "Task 5.4.12: Calculate the training and test accuracy scores for model."
      ],
      "metadata": {
        "id": "yZQ6fj3efPBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_train = model.score(X_train, y_train)\n",
        "acc_test = model.score(X_test, y_test)\n",
        "\n",
        "print(\"Training Accuracy:\", round(acc_train, 4))\n",
        "print(\"Validation Accuracy:\", round(acc_test, 4))"
      ],
      "metadata": {
        "id": "74Q8BLdTfQMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 5.4.13: Plot a confusion matrix that shows how your best model performs on your test set."
      ],
      "metadata": {
        "id": "a2pD3ww0fSjA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot confusion matrix\n",
        "ConfusionMatrixDisplay.from_estimator(model, X_test, y_test);"
      ],
      "metadata": {
        "id": "G5NahaoXfT35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This matrix is a great reminder of how imbalanced our data is, and of why accuracy isn't always the best metric for judging whether or not a model is giving us what we want. After all, if 95% of the companies in our dataset didn't go bankrupt, all the model has to do is always predict {\"bankrupt\": False}, and it'll be right 95% of the time. The accuracy score will be amazing, but it won't tell us what we really need to know.\n",
        "\n",
        "Instead, we can evaluate our model using two new metrics: precision and recall. The precision score is important when we want our model to only predict that a company will go bankrupt if its very confident in its prediction. The recall score is important if we want to make sure to identify all the companies that will go bankrupt, even if that means being incorrect sometimes.\n",
        "\n",
        "Let's start with a report you can create with scikit-learn to calculate both metrics. Then we'll look at them one-by-one using a visualization tool we've built especially for the Data Science Lab."
      ],
      "metadata": {
        "id": "4SK_BDuRfVaJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 5.4.14: Print the classification report for your model, using the test set."
      ],
      "metadata": {
        "id": "mjhyMbHUfW5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print classification report\n",
        "print(classification_report(y_test, model.predict(X_test)))"
      ],
      "metadata": {
        "id": "PCU0Mx29fYcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(X_test)[:5]"
      ],
      "metadata": {
        "id": "vm90b8OmfZzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict_proba(X_test)[:5, -1]"
      ],
      "metadata": {
        "id": "UsIO3SIHfaxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ask 5.4.15: Run the cell below to load the confusion matrix widget."
      ],
      "metadata": {
        "id": "QXd2QdpjfceW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c = ConfusionMatrixWidget(model, X_test, y_test)\n",
        "c.show()"
      ],
      "metadata": {
        "id": "tYSpGD4xfdnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you move the probability threshold, you can see that there's a tradeoff between precision and recall. That is, as one gets better, the other suffers. As a data scientist, you'll often need to decide whether you want a model with better precision or better recall. What you choose will depend on how to intend to use your model.\n",
        "\n",
        "Let's look at two examples, one where recall is the priority and one where precision is more important. First, let's say you work for a regulatory agency in the European Union that assists companies and investors navigate insolvency proceedings. You want to build a model to predict which companies could go bankrupt so that you can send debtors information about filing for legal protection before their company becomes insolvent. The administrative costs of sending information to a company is €500. The legal costs to the European court system if a company doesn't file for protection before bankruptcy is €50,000.\n",
        "\n",
        "For a model like this, we want to focus on recall, because recall is all about quantity. A model that prioritizes recall will cast the widest possible net, which is the way to approach this problem. We want to send information to as many potentially-bankrupt companies as possible, because it costs a lot less to send information to a company that might not become insolvent than it does to skip a company that does."
      ],
      "metadata": {
        "id": "tEZ8KAGaff88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 5.4.16: Run the cell below, and use the slider to change the probability threshold of your model. What relationship do you see between changes in the threshold and changes in wasted administrative and legal costs? In your opinion, which is more important for this model: high precision or high recall?"
      ],
      "metadata": {
        "id": "vqcBzws9fhms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c.show_eu()"
      ],
      "metadata": {
        "id": "qxJuBchkfiqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the second example, let's say we work at a private equity firm that purchases distressed businesses, improve them, and then sells them for a profit. You want to build a model to predict which companies will go bankrupt so that you can purchase them ahead of your competitors. If the firm purchases a company that is indeed insolvent, it can make a profit of €100 million or more. But if it purchases a company that isn't insolvent and can't be resold at a profit, the firm will lose €250 million.\n",
        "\n",
        "For a model like this, we want to focus on precision. If we're trying to maximize our profit, the quality of our predictions is much more important than the quantity of our predictions. It's not a big deal if we don't catch every single insolvent company, but it's definitely a big deal if the companies we catch don't end up becoming insolvent.\n",
        "\n",
        "This time we're going to build the visualization together."
      ],
      "metadata": {
        "id": "IAdRAOgwfkn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "threshold=0.2\n",
        "y_pred_proba=model.predict_proba(X_test)[:, -1]\n",
        "y_pred=y_pred_proba>threshold\n",
        "conf_matrix=confusion_matrix(y_test, y_pred)\n",
        "tn, fp, fn, tp=conf_matrix.ravel()\n",
        "print(f\"Profit: €{tp*100_000_000}\")\n",
        "print(f\"Losses: €{fp*250_000_000}\")\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred, colorbar=False)\n"
      ],
      "metadata": {
        "id": "Iq2Mdwlmfl2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 5.4.17: Create an interactive dashboard that shows how company profit and losses change in relationship to your model's probability threshold. Start with the make_cnf_matrix function, which should calculate and print profit/losses, and display a confusion matrix. Then create a FloatSlider thresh_widget that ranges from 0 to 1. Finally combine your function and slider in the interact function."
      ],
      "metadata": {
        "id": "dp5CO4mZfnww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_cnf_matrix(threshold):\n",
        "    y_pred_proba=model.predict_proba(X_test)[:, -1]\n",
        "    y_pred=y_pred_proba>threshold\n",
        "    conf_matrix=confusion_matrix(y_test, y_pred)\n",
        "    tn, fp, fn, tp=conf_matrix.ravel()\n",
        "    print(f\"Total Profit: €{tp*100_000_000}\")\n",
        "    print(f\"Total Losses: €{fp*250_000_000}\")\n",
        "    ConfusionMatrixDisplay.from_predictions(y_test, y_pred, colorbar=False)\n",
        "\n",
        "thresh_widget = widgets.FloatSlider(min=0, max=1, value=0.5, step=0.05)\n",
        "\n",
        "interact(make_cnf_matrix, threshold=thresh_widget);"
      ],
      "metadata": {
        "id": "kkuPdauPfo-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Communicate\n",
        "Almost there! Save the best model so we can share it with other people, then put it all together with what we learned in the last lesson.\n",
        "\n",
        "Task 5.4.18: Using a context manager, save your best-performing model to a file named \"model-5-4.pkl\"."
      ],
      "metadata": {
        "id": "ioL_6su0fqhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "with open(\"model-5-4.pkl\", \"wb\") as f:\n",
        "    pickle.dump(model, f)"
      ],
      "metadata": {
        "id": "ffV7-sczfr60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 5.4.19: Open the file my_predictor_lesson.py, add the wrangle and make_predictions functions from the last lesson, and add all the necessary import statements to the top of the file. Once you're done, save the file. You can check that the contents are correct by running the cell below."
      ],
      "metadata": {
        "id": "j1gV-n4Cfto7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "cat my_predictor_lesson.py"
      ],
      "metadata": {
        "id": "OrbE3YvNfumj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 5.4.20: Import your make_predictions function from your my_predictor module, and use the code below to make sure it works as expected. Once you're satisfied, submit it to the grader."
      ],
      "metadata": {
        "id": "Gsg_pFg-fwhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your module\n",
        "from my_predictor_lesson import make_predictions\n",
        "\n",
        "# Generate predictions\n",
        "y_test_pred = make_predictions(\n",
        "    data_filepath=\"data/poland-bankruptcy-data-2009-mvp-features.json.gz\",\n",
        "    model_filepath=\"model-5-4.pkl\",\n",
        ")\n",
        "\n",
        "print(\"predictions shape:\", y_test_pred.shape)\n",
        "y_test_pred.head()"
      ],
      "metadata": {
        "id": "LEw7HY_2fxg7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}